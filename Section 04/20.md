
# **20 Tokens, Cost & Context Window**

> Every LLM works on **tokens**, not words.
> Understanding tokens is **mandatory** for cost control, performance, and agent design.

---

## **20.1 What is a Token?**

A **token** is a **piece of text** processed by an LLM.

It can be:

* A whole word
* Part of a word
* A symbol
* A number
* Punctuation

### Examples

| Text           | Tokens             |
| -------------- | ------------------ |
| `Hello`        | 1                  |
| `ChatGPT`      | 1‚Äì2                |
| `unbelievable` | `un + believable`  |
| `JSON-first`   | `JSON + - + first` |

üëâ **Tokens ‚â† Words**

---

## **20.2 Why LLMs Use Tokens**

LLMs:

* Learn language statistically
* Predict the **next token**
* Operate on token sequences

### Core LLM Task

```b
Given tokens so far ‚Üí predict next token
```

---

## **20.3 Input Tokens vs Output Tokens**

### 1Ô∏è‚É£ Input Tokens

Include:

* System prompt
* Instructions
* Conversation history
* Tool definitions
* User query
* JSON schemas

### 2Ô∏è‚É£ Output Tokens

Include:

* Model‚Äôs response
* Tool-call JSON
* Final answer

### Cost = **Input tokens + Output tokens**

---

## **20.4 Token Cost (VERY IMPORTANT ‚≠ê)**

LLMs are **priced per token**.

### General Pricing Logic

| Type          | Cost           |
| ------------- | -------------- |
| Input tokens  | Cheaper        |
| Output tokens | More expensive |

üëâ Long prompts = higher cost
üëâ Long outputs = higher cost

---

### Example Cost Calculation

If:

* Input = 2,000 tokens
* Output = 500 tokens

Then:

```b
Total tokens = 2,500
Total cost ‚àù 2,500 tokens
```

---

## **20.5 Context Window (Memory Limit üß†)**

### Definition

**Context window** = Maximum number of tokens the model can process **at once**.

Includes:

* System prompt
* History
* User input
* Tool schemas
* Model output

---

### Context Window Example

| Model      | Context Window  |
| ---------- | --------------- |
| Small LLM  | 8k tokens       |
| Modern LLM | 32k‚Äì128k tokens |

‚ö†Ô∏è If you exceed the context window:

* Old messages are truncated
* Or request fails

---

## **20.6 Why Context Window Matters for Agents**

Agents often include:

* Long instructions
* Tool definitions
* JSON schemas
* Memory history
* Multi-step reasoning

üëâ Context fills up **very fast**.

### Agent Failure Example

* Tool schema too long
* Conversation history too big
* JSON output gets cut mid-way ‚ùå

---

## **20.7 Token Explosion in JSON-First Systems**

JSON is **verbose**.

### Example

```json
{
  "action": "call_tool",
  "tool": "search_api",
  "arguments": {
    "query": "LangChain agents"
  }
}
```

üëâ More structure = more tokens
üëâ More safety = more cost

---

## **20.8 Token Optimization Strategies (Advanced)**

### 1Ô∏è‚É£ Minimize System Prompts

* Avoid repeated instructions
* Keep rules concise

---

### 2Ô∏è‚É£ Compress Tool Schemas

* Remove unused fields
* Shorten descriptions

---

### 3Ô∏è‚É£ Control Output Size

* Limit response length
* Enforce strict JSON schema

---

### 4Ô∏è‚É£ Use Memory Externalization

* Store long-term memory in DB
* Inject only relevant context

---

### 5Ô∏è‚É£ Sliding Window Strategy

* Keep only recent messages
* Summarize older context

---

## **20.9 Context Window vs Memory (Exam Favorite ‚≠ê)**

| Context Window | Memory    |
| -------------- | --------- |
| Short-term     | Long-term |
| Limited        | Unlimited |
| Token-based    | DB-based  |
| Expensive      | Cheap     |

üëâ **Memory ‚â† Context**

---

## **20.10 Token & Cost Trade-offs**

| Goal            | Strategy                    |
| --------------- | --------------------------- |
| Lower cost      | Short prompts               |
| More accuracy   | Larger context              |
| Faster response | Smaller outputs             |
| Better agents   | Structured + optimized JSON |

---

## **20.11 Common Exam Questions**

**Q1:** What is a token?
**Ans:** Smallest unit of text processed by an LLM.

**Q2:** What counts toward context window?
**Ans:** All input + output tokens in a single request.

**Q3:** Why do agents cost more?
**Ans:** Tool schemas, memory, JSON outputs increase token usage.

---

## **20.12 Section 4.1 Summary**

* LLMs operate on tokens, not words
* Cost is token-based
* Context window is finite
* JSON-first systems increase token usage
* Optimization is required for production agents

---
